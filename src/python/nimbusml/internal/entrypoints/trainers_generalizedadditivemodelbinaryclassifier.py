# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
Trainers.GeneralizedAdditiveModelBinaryClassifier
"""

import numbers

from ..utils.entrypoints import EntryPoint
from ..utils.utils import try_set, unlist


def trainers_generalizedadditivemodelbinaryclassifier(
        training_data,
        predictor_model=None,
        number_of_iterations=9500,
        feature_column_name='Features',
        minimum_example_count_per_leaf=10,
        label_column_name='Label',
        learning_rate=0.002,
        example_weight_column_name=None,
        normalize_features='Auto',
        caching='Auto',
        unbalanced_sets=False,
        entropy_coefficient=0.0,
        gain_confidence_level=0,
        number_of_threads=None,
        disk_transpose=None,
        maximum_bin_count_per_feature=255,
        maximum_tree_output=float("inf"),
        get_derivatives_sample_rate=1,
        seed=123,
        feature_flocks=True,
        enable_pruning=True,
        **params):
    """
    **Description**
        Trains a gradient boosted stump per feature, on all features
        simultaneously, to fit target values using least-squares. It
        mantains no interactions between features.

    :param number_of_iterations: Total number of iterations over all
        features (inputs).
    :param training_data: The data to be used for training (inputs).
    :param feature_column_name: Column to use for features (inputs).
    :param minimum_example_count_per_leaf: Minimum number of training
        instances required to form a partition (inputs).
    :param label_column_name: Column to use for labels (inputs).
    :param learning_rate: The learning rate (inputs).
    :param example_weight_column_name: Column to use for example
        weight (inputs).
    :param normalize_features: Normalize option for the feature
        column (inputs).
    :param caching: Whether trainer should cache input training data
        (inputs).
    :param unbalanced_sets: Should we use derivatives optimized for
        unbalanced sets (inputs).
    :param entropy_coefficient: The entropy (regularization)
        coefficient between 0 and 1 (inputs).
    :param gain_confidence_level: Tree fitting gain confidence
        requirement (should be in the range [0,1) ). (inputs).
    :param number_of_threads: The number of threads to use (inputs).
    :param disk_transpose: Whether to utilize the disk or the data's
        native transposition facilities (where applicable) when
        performing the transpose (inputs).
    :param maximum_bin_count_per_feature: Maximum number of distinct
        values (bins) per feature (inputs).
    :param maximum_tree_output: Upper bound on absolute value of
        single output (inputs).
    :param get_derivatives_sample_rate: Sample each query 1 in k
        times in the GetDerivatives function (inputs).
    :param seed: The seed of the random number generator (inputs).
    :param feature_flocks: Whether to collectivize features during
        dataset preparation to speed up training (inputs).
    :param enable_pruning: Enable post-training pruning to avoid
        overfitting. (a validation set is required) (inputs).
    :param predictor_model: The trained model (outputs).
    """

    entrypoint_name = 'Trainers.GeneralizedAdditiveModelBinaryClassifier'
    inputs = {}
    outputs = {}

    if number_of_iterations is not None:
        inputs['NumberOfIterations'] = try_set(
            obj=number_of_iterations,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if training_data is not None:
        inputs['TrainingData'] = try_set(
            obj=training_data,
            none_acceptable=False,
            is_of_type=str)
    if feature_column_name is not None:
        inputs['FeatureColumnName'] = try_set(
            obj=feature_column_name,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if minimum_example_count_per_leaf is not None:
        inputs['MinimumExampleCountPerLeaf'] = try_set(
            obj=minimum_example_count_per_leaf,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if label_column_name is not None:
        inputs['LabelColumnName'] = try_set(
            obj=label_column_name,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if learning_rate is not None:
        inputs['LearningRate'] = try_set(
            obj=learning_rate,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if example_weight_column_name is not None:
        inputs['ExampleWeightColumnName'] = try_set(
            obj=example_weight_column_name,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if normalize_features is not None:
        inputs['NormalizeFeatures'] = try_set(
            obj=normalize_features,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'No',
                'Warn',
                'Auto',
                'Yes'])
    if caching is not None:
        inputs['Caching'] = try_set(
            obj=caching,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'Auto',
                'Memory',
                'None'])
    if unbalanced_sets is not None:
        inputs['UnbalancedSets'] = try_set(
            obj=unbalanced_sets,
            none_acceptable=True,
            is_of_type=bool)
    if entropy_coefficient is not None:
        inputs['EntropyCoefficient'] = try_set(
            obj=entropy_coefficient,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if gain_confidence_level is not None:
        inputs['GainConfidenceLevel'] = try_set(
            obj=gain_confidence_level,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if number_of_threads is not None:
        inputs['NumberOfThreads'] = try_set(
            obj=number_of_threads,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if disk_transpose is not None:
        inputs['DiskTranspose'] = try_set(
            obj=disk_transpose,
            none_acceptable=True,
            is_of_type=bool)
    if maximum_bin_count_per_feature is not None:
        inputs['MaximumBinCountPerFeature'] = try_set(
            obj=maximum_bin_count_per_feature,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if maximum_tree_output is not None:
        inputs['MaximumTreeOutput'] = try_set(
            obj=maximum_tree_output,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if get_derivatives_sample_rate is not None:
        inputs['GetDerivativesSampleRate'] = try_set(
            obj=get_derivatives_sample_rate,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if seed is not None:
        inputs['Seed'] = try_set(
            obj=seed,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if feature_flocks is not None:
        inputs['FeatureFlocks'] = try_set(
            obj=feature_flocks,
            none_acceptable=True,
            is_of_type=bool)
    if enable_pruning is not None:
        inputs['EnablePruning'] = try_set(
            obj=enable_pruning,
            none_acceptable=True,
            is_of_type=bool)
    if predictor_model is not None:
        outputs['PredictorModel'] = try_set(
            obj=predictor_model, none_acceptable=False, is_of_type=str)

    input_variables = {
        x for x in unlist(inputs.values())
        if isinstance(x, str) and x.startswith("$")}
    output_variables = {
        x for x in unlist(outputs.values())
        if isinstance(x, str) and x.startswith("$")}

    entrypoint = EntryPoint(
        name=entrypoint_name, inputs=inputs, outputs=outputs,
        input_variables=input_variables,
        output_variables=output_variables)
    return entrypoint
