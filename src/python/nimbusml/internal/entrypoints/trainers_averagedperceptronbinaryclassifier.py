# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
Trainers.AveragedPerceptronBinaryClassifier
"""

import numbers

from ..utils.entrypoints import EntryPoint
from ..utils.utils import try_set, unlist


def trainers_averagedperceptronbinaryclassifier(
        training_data,
        predictor_model=None,
        feature_column='Features',
        label_column='Label',
        normalize_features='Auto',
        caching='Auto',
        loss_function=None,
        learning_rate=1.0,
        decrease_learning_rate=False,
        l2_regularizer_weight=0.0,
        num_iterations=1,
        init_wts_diameter=0.0,
        calibrator=None,
        max_calibration_examples=1000000,
        reset_weights_after_x_examples=None,
        do_lazy_updates=True,
        recency_gain=0.0,
        recency_gain_multi=False,
        averaged=True,
        averaged_tolerance=0.01,
        initial_weights=None,
        shuffle=True,
        streaming_cache_size=1000000,
        **params):
    """
    **Description**
        Averaged Perceptron Binary Classifier.

    :param training_data: The data to be used for training (inputs).
    :param feature_column: Column to use for features (inputs).
    :param label_column: Column to use for labels (inputs).
    :param normalize_features: Normalize option for the feature
        column (inputs).
    :param caching: Whether learner should cache input training data
        (inputs).
    :param loss_function: Loss Function (inputs).
    :param learning_rate: Learning rate (inputs).
    :param decrease_learning_rate: Decrease learning rate (inputs).
    :param l2_regularizer_weight: L2 Regularization Weight (inputs).
    :param num_iterations: Number of iterations (inputs).
    :param init_wts_diameter: Init weights diameter (inputs).
    :param calibrator: The calibrator kind to apply to the predictor.
        Specify null for no calibration (inputs).
    :param max_calibration_examples: The maximum number of examples
        to use when training the calibrator (inputs).
    :param reset_weights_after_x_examples: Number of examples after
        which weights will be reset to the current average (inputs).
    :param do_lazy_updates: Instead of updating averaged weights on
        every example, only update when loss is nonzero (inputs).
    :param recency_gain: Extra weight given to more recent updates
        (inputs).
    :param recency_gain_multi: Whether Recency Gain is multiplicative
        (vs. additive) (inputs).
    :param averaged: Do averaging? (inputs).
    :param averaged_tolerance: The inexactness tolerance for
        averaging (inputs).
    :param initial_weights: Initial Weights and bias, comma-separated
        (inputs).
    :param shuffle: Whether to shuffle for each training iteration
        (inputs).
    :param streaming_cache_size: Size of cache when trained in Scope
        (inputs).
    :param predictor_model: The trained model (outputs).
    """

    entrypoint_name = 'Trainers.AveragedPerceptronBinaryClassifier'
    inputs = {}
    outputs = {}

    if training_data is not None:
        inputs['TrainingData'] = try_set(
            obj=training_data,
            none_acceptable=False,
            is_of_type=str)
    if feature_column is not None:
        inputs['FeatureColumn'] = try_set(
            obj=feature_column,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if label_column is not None:
        inputs['LabelColumn'] = try_set(
            obj=label_column,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if normalize_features is not None:
        inputs['NormalizeFeatures'] = try_set(
            obj=normalize_features,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'No',
                'Warn',
                'Auto',
                'Yes'])
    if caching is not None:
        inputs['Caching'] = try_set(
            obj=caching,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'Auto',
                'Memory',
                'Disk',
                'None'])
    if loss_function is not None:
        inputs['LossFunction'] = try_set(
            obj=loss_function,
            none_acceptable=True,
            is_of_type=dict)
    if learning_rate is not None:
        inputs['LearningRate'] = try_set(
            obj=learning_rate,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if decrease_learning_rate is not None:
        inputs['DecreaseLearningRate'] = try_set(
            obj=decrease_learning_rate, none_acceptable=True, is_of_type=bool)
    if l2_regularizer_weight is not None:
        inputs['L2RegularizerWeight'] = try_set(
            obj=l2_regularizer_weight,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if num_iterations is not None:
        inputs['NumIterations'] = try_set(
            obj=num_iterations,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if init_wts_diameter is not None:
        inputs['InitWtsDiameter'] = try_set(
            obj=init_wts_diameter,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if calibrator is not None:
        inputs['Calibrator'] = try_set(
            obj=calibrator,
            none_acceptable=True,
            is_of_type=dict)
    if max_calibration_examples is not None:
        inputs['MaxCalibrationExamples'] = try_set(
            obj=max_calibration_examples,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if reset_weights_after_x_examples is not None:
        inputs['ResetWeightsAfterXExamples'] = try_set(
            obj=reset_weights_after_x_examples,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if do_lazy_updates is not None:
        inputs['DoLazyUpdates'] = try_set(
            obj=do_lazy_updates,
            none_acceptable=True,
            is_of_type=bool)
    if recency_gain is not None:
        inputs['RecencyGain'] = try_set(
            obj=recency_gain,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if recency_gain_multi is not None:
        inputs['RecencyGainMulti'] = try_set(
            obj=recency_gain_multi,
            none_acceptable=True,
            is_of_type=bool)
    if averaged is not None:
        inputs['Averaged'] = try_set(
            obj=averaged,
            none_acceptable=True,
            is_of_type=bool)
    if averaged_tolerance is not None:
        inputs['AveragedTolerance'] = try_set(
            obj=averaged_tolerance,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if initial_weights is not None:
        inputs['InitialWeights'] = try_set(
            obj=initial_weights,
            none_acceptable=True,
            is_of_type=str)
    if shuffle is not None:
        inputs['Shuffle'] = try_set(
            obj=shuffle,
            none_acceptable=True,
            is_of_type=bool)
    if streaming_cache_size is not None:
        inputs['StreamingCacheSize'] = try_set(
            obj=streaming_cache_size,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if predictor_model is not None:
        outputs['PredictorModel'] = try_set(
            obj=predictor_model, none_acceptable=False, is_of_type=str)

    input_variables = {
        x for x in unlist(inputs.values())
        if isinstance(x, str) and x.startswith("$")}
    output_variables = {
        x for x in unlist(outputs.values())
        if isinstance(x, str) and x.startswith("$")}

    entrypoint = EntryPoint(
        name=entrypoint_name, inputs=inputs, outputs=outputs,
        input_variables=input_variables,
        output_variables=output_variables)
    return entrypoint
