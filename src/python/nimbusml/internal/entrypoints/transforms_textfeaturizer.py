# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
Transforms.TextFeaturizer
"""


from ..utils.entrypoints import EntryPoint
from ..utils.utils import try_set, unlist
from ._ngramextractor_ngram import n_gram


def transforms_textfeaturizer(
        column,
        data,
        output_data=None,
        model=None,
        language='English',
        stop_words_remover=None,
        text_case='Lower',
        keep_diacritics=False,
        keep_punctuations=True,
        keep_numbers=True,
        output_tokens_column_name=None,
        dictionary=None,
        word_feature_extractor=n_gram(
            max_num_terms=[10000000]),
    char_feature_extractor=n_gram(
            ngram_length=3,
            all_lengths=False,
            max_num_terms=[10000000]),
        vector_normalizer='L2',
        **params):
    """
    **Description**
        A transform that turns a collection of text documents into numerical
        feature vectors. The feature vectors are normalized counts of
        (word and/or character) n-grams in a given tokenized text.

    :param column: New column definition (optional form: name:srcs).
        (inputs).
    :param data: Input dataset (inputs).
    :param language: Dataset language or 'AutoDetect' to detect
        language per row. (inputs).
    :param stop_words_remover: Stopwords remover. (inputs).
    :param text_case: Casing text using the rules of the invariant
        culture. (inputs).
    :param keep_diacritics: Whether to keep diacritical marks or
        remove them. (inputs).
    :param keep_punctuations: Whether to keep punctuation marks or
        remove them. (inputs).
    :param keep_numbers: Whether to keep numbers or remove them.
        (inputs).
    :param output_tokens_column_name: Column containing the
        transformed text tokens. (inputs).
    :param dictionary: A dictionary of whitelisted terms. (inputs).
    :param word_feature_extractor: Ngram feature extractor to use for
        words (WordBag/WordHashBag). (inputs).
    :param char_feature_extractor: Ngram feature extractor to use for
        characters (WordBag/WordHashBag). (inputs).
    :param vector_normalizer: Normalize vectors (rows) individually
        by rescaling them to unit norm. (inputs).
    :param output_data: Transformed dataset (outputs).
    :param model: Transform model (outputs).
    """

    entrypoint_name = 'Transforms.TextFeaturizer'
    inputs = {}
    outputs = {}

    if column is not None:
        inputs['Column'] = try_set(
            obj=column,
            none_acceptable=False,
            is_of_type=dict,
            is_column=True,
            field_names=[
                'Name',
                'Source'])
    if data is not None:
        inputs['Data'] = try_set(
            obj=data,
            none_acceptable=False,
            is_of_type=str)
    if language is not None:
        inputs['Language'] = try_set(
            obj=language,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'English',
                'French',
                'German',
                'Dutch',
                'Italian',
                'Spanish',
                'Japanese'])
    if stop_words_remover is not None:
        inputs['StopWordsRemover'] = try_set(
            obj=stop_words_remover,
            none_acceptable=True,
            is_of_type=dict)
    if text_case is not None:
        inputs['TextCase'] = try_set(
            obj=text_case,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'Lower',
                'Upper',
                'None'])
    if keep_diacritics is not None:
        inputs['KeepDiacritics'] = try_set(
            obj=keep_diacritics,
            none_acceptable=True,
            is_of_type=bool)
    if keep_punctuations is not None:
        inputs['KeepPunctuations'] = try_set(
            obj=keep_punctuations,
            none_acceptable=True,
            is_of_type=bool)
    if keep_numbers is not None:
        inputs['KeepNumbers'] = try_set(
            obj=keep_numbers,
            none_acceptable=True,
            is_of_type=bool)
    if output_tokens_column_name is not None:
        inputs['OutputTokensColumnName'] = try_set(
            obj=output_tokens_column_name,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if dictionary is not None:
        inputs['Dictionary'] = try_set(
            obj=dictionary,
            none_acceptable=True,
            is_of_type=dict,
            field_names=[
                'Term',
                'Sort',
                'DropUnknowns'])
    inputs['WordFeatureExtractor'] = try_set(
        obj=word_feature_extractor,
        none_acceptable=True,
        is_of_type=dict)
    inputs['CharFeatureExtractor'] = try_set(
        obj=char_feature_extractor,
        none_acceptable=True,
        is_of_type=dict)
    if vector_normalizer is not None:
        inputs['VectorNormalizer'] = try_set(
            obj=vector_normalizer,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'None',
                'L1',
                'L2',
                'Infinity'])
    if output_data is not None:
        outputs['OutputData'] = try_set(
            obj=output_data,
            none_acceptable=False,
            is_of_type=str)
    if model is not None:
        outputs['Model'] = try_set(
            obj=model,
            none_acceptable=False,
            is_of_type=str)

    input_variables = {
        x for x in unlist(inputs.values())
        if isinstance(x, str) and x.startswith("$")}
    output_variables = {
        x for x in unlist(outputs.values())
        if isinstance(x, str) and x.startswith("$")}

    entrypoint = EntryPoint(
        name=entrypoint_name, inputs=inputs, outputs=outputs,
        input_variables=input_variables,
        output_variables=output_variables)
    return entrypoint
