# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
Trainers.FastForestBinaryClassifier
"""

import numbers

from ..utils.entrypoints import EntryPoint
from ..utils.utils import try_set, unlist


def trainers_fastforestbinaryclassifier(
        training_data,
        predictor_model=None,
        num_trees=100,
        num_leaves=20,
        feature_column='Features',
        min_documents_in_leafs=10,
        label_column='Label',
        weight_column=None,
        group_id_column=None,
        normalize_features='Auto',
        caching='Auto',
        max_tree_output=100.0,
        calibrator=None,
        max_calibration_examples=1000000,
        quantile_sample_count=100,
        parallel_trainer=None,
        num_threads=None,
        rng_seed=123,
        feature_select_seed=123,
        entropy_coefficient=0.0,
        histogram_pool_size=-1,
        disk_transpose=None,
        feature_flocks=True,
        categorical_split=False,
        max_categorical_groups_per_node=64,
        max_categorical_split_points=64,
        min_docs_percentage_for_categorical_split=0.001,
        min_docs_for_categorical_split=100,
        bias=0.0,
        bundling='None',
        max_bins=255,
        sparsify_threshold=0.7,
        feature_first_use_penalty=0.0,
        feature_reuse_penalty=0.0,
        gain_confidence_level=0.0,
        softmax_temperature=0.0,
        execution_times=False,
        feature_fraction=0.7,
        bagging_size=1,
        bagging_train_fraction=0.7,
        split_fraction=0.7,
        smoothing=0.0,
        allow_empty_trees=True,
        feature_compression_level=1,
        compress_ensemble=False,
        max_trees_after_compression=-1,
        print_test_graph=False,
        print_train_valid_graph=False,
        test_frequency=2147483647,
        **params):
    """
    **Description**
        Uses a random forest learner to perform binary classification.

    :param num_trees: Total number of decision trees to create in the
        ensemble (inputs).
    :param training_data: The data to be used for training (inputs).
    :param num_leaves: The max number of leaves in each regression
        tree (inputs).
    :param feature_column: Column to use for features (inputs).
    :param min_documents_in_leafs: The minimal number of documents
        allowed in a leaf of a regression tree, out of the subsampled
        data (inputs).
    :param label_column: Column to use for labels (inputs).
    :param weight_column: Column to use for example weight (inputs).
    :param group_id_column: Column to use for example groupId
        (inputs).
    :param normalize_features: Normalize option for the feature
        column (inputs).
    :param caching: Whether learner should cache input training data
        (inputs).
    :param max_tree_output: Upper bound on absolute value of single
        tree output (inputs).
    :param calibrator: The calibrator kind to apply to the predictor.
        Specify null for no calibration (inputs).
    :param max_calibration_examples: The maximum number of examples
        to use when training the calibrator (inputs).
    :param quantile_sample_count: Number of labels to be sampled from
        each leaf to make the distribtuion (inputs).
    :param parallel_trainer: Allows to choose Parallel FastTree
        Learning Algorithm (inputs).
    :param num_threads: The number of threads to use (inputs).
    :param rng_seed: The seed of the random number generator
        (inputs).
    :param feature_select_seed: The seed of the active feature
        selection (inputs).
    :param entropy_coefficient: The entropy (regularization)
        coefficient between 0 and 1 (inputs).
    :param histogram_pool_size: The number of histograms in the pool
        (between 2 and numLeaves) (inputs).
    :param disk_transpose: Whether to utilize the disk or the data's
        native transposition facilities (where applicable) when
        performing the transpose (inputs).
    :param feature_flocks: Whether to collectivize features during
        dataset preparation to speed up training (inputs).
    :param categorical_split: Whether to do split based on multiple
        categorical feature values. (inputs).
    :param max_categorical_groups_per_node: Maximum categorical split
        groups to consider when splitting on a categorical feature.
        Split groups are a collection of split points. This is used
        to reduce overfitting when there many categorical features.
        (inputs).
    :param max_categorical_split_points: Maximum categorical split
        points to consider when splitting on a categorical feature.
        (inputs).
    :param min_docs_percentage_for_categorical_split: Minimum
        categorical docs percentage in a bin to consider for a split.
        (inputs).
    :param min_docs_for_categorical_split: Minimum categorical doc
        count in a bin to consider for a split. (inputs).
    :param bias: Bias for calculating gradient for each feature bin
        for a categorical feature. (inputs).
    :param bundling: Bundle low population bins. Bundle.None(0): no
        bundling, Bundle.AggregateLowPopulation(1): Bundle low
        population, Bundle.Adjacent(2): Neighbor low population
        bundle. (inputs).
    :param max_bins: Maximum number of distinct values (bins) per
        feature (inputs).
    :param sparsify_threshold: Sparsity level needed to use sparse
        feature representation (inputs).
    :param feature_first_use_penalty: The feature first use penalty
        coefficient (inputs).
    :param feature_reuse_penalty: The feature re-use penalty
        (regularization) coefficient (inputs).
    :param gain_confidence_level: Tree fitting gain confidence
        requirement (should be in the range [0,1) ). (inputs).
    :param softmax_temperature: The temperature of the randomized
        softmax distribution for choosing the feature (inputs).
    :param execution_times: Print execution time breakdown to stdout
        (inputs).
    :param feature_fraction: The fraction of features (chosen
        randomly) to use on each iteration (inputs).
    :param bagging_size: Number of trees in each bag (0 for disabling
        bagging) (inputs).
    :param bagging_train_fraction: Percentage of training examples
        used in each bag (inputs).
    :param split_fraction: The fraction of features (chosen randomly)
        to use on each split (inputs).
    :param smoothing: Smoothing paramter for tree regularization
        (inputs).
    :param allow_empty_trees: When a root split is impossible, allow
        training to proceed (inputs).
    :param feature_compression_level: The level of feature
        compression to use (inputs).
    :param compress_ensemble: Compress the tree Ensemble (inputs).
    :param max_trees_after_compression: Maximum Number of trees after
        compression (inputs).
    :param print_test_graph: Print metrics graph for the first test
        set (inputs).
    :param print_train_valid_graph: Print Train and Validation
        metrics in graph (inputs).
    :param test_frequency: Calculate metric values for
        train/valid/test every k rounds (inputs).
    :param predictor_model: The trained model (outputs).
    """

    entrypoint_name = 'Trainers.FastForestBinaryClassifier'
    inputs = {}
    outputs = {}

    if num_trees is not None:
        inputs['NumTrees'] = try_set(
            obj=num_trees,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if training_data is not None:
        inputs['TrainingData'] = try_set(
            obj=training_data,
            none_acceptable=False,
            is_of_type=str)
    if num_leaves is not None:
        inputs['NumLeaves'] = try_set(
            obj=num_leaves,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if feature_column is not None:
        inputs['FeatureColumn'] = try_set(
            obj=feature_column,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if min_documents_in_leafs is not None:
        inputs['MinDocumentsInLeafs'] = try_set(
            obj=min_documents_in_leafs,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if label_column is not None:
        inputs['LabelColumn'] = try_set(
            obj=label_column,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if weight_column is not None:
        inputs['WeightColumn'] = try_set(
            obj=weight_column,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if group_id_column is not None:
        inputs['GroupIdColumn'] = try_set(
            obj=group_id_column,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if normalize_features is not None:
        inputs['NormalizeFeatures'] = try_set(
            obj=normalize_features,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'No',
                'Warn',
                'Auto',
                'Yes'])
    if caching is not None:
        inputs['Caching'] = try_set(
            obj=caching,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'Auto',
                'Memory',
                'Disk',
                'None'])
    if max_tree_output is not None:
        inputs['MaxTreeOutput'] = try_set(
            obj=max_tree_output,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if calibrator is not None:
        inputs['Calibrator'] = try_set(
            obj=calibrator,
            none_acceptable=True,
            is_of_type=dict)
    if max_calibration_examples is not None:
        inputs['MaxCalibrationExamples'] = try_set(
            obj=max_calibration_examples,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if quantile_sample_count is not None:
        inputs['QuantileSampleCount'] = try_set(
            obj=quantile_sample_count,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if parallel_trainer is not None:
        inputs['ParallelTrainer'] = try_set(
            obj=parallel_trainer,
            none_acceptable=True,
            is_of_type=dict)
    if num_threads is not None:
        inputs['NumThreads'] = try_set(
            obj=num_threads,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if rng_seed is not None:
        inputs['RngSeed'] = try_set(
            obj=rng_seed,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if feature_select_seed is not None:
        inputs['FeatureSelectSeed'] = try_set(
            obj=feature_select_seed,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if entropy_coefficient is not None:
        inputs['EntropyCoefficient'] = try_set(
            obj=entropy_coefficient,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if histogram_pool_size is not None:
        inputs['HistogramPoolSize'] = try_set(
            obj=histogram_pool_size,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if disk_transpose is not None:
        inputs['DiskTranspose'] = try_set(
            obj=disk_transpose,
            none_acceptable=True,
            is_of_type=bool)
    if feature_flocks is not None:
        inputs['FeatureFlocks'] = try_set(
            obj=feature_flocks,
            none_acceptable=True,
            is_of_type=bool)
    if categorical_split is not None:
        inputs['CategoricalSplit'] = try_set(
            obj=categorical_split,
            none_acceptable=True,
            is_of_type=bool)
    if max_categorical_groups_per_node is not None:
        inputs['MaxCategoricalGroupsPerNode'] = try_set(
            obj=max_categorical_groups_per_node,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if max_categorical_split_points is not None:
        inputs['MaxCategoricalSplitPoints'] = try_set(
            obj=max_categorical_split_points,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if min_docs_percentage_for_categorical_split is not None:
        inputs['MinDocsPercentageForCategoricalSplit'] = try_set(
            obj=min_docs_percentage_for_categorical_split,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if min_docs_for_categorical_split is not None:
        inputs['MinDocsForCategoricalSplit'] = try_set(
            obj=min_docs_for_categorical_split,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if bias is not None:
        inputs['Bias'] = try_set(
            obj=bias,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if bundling is not None:
        inputs['Bundling'] = try_set(
            obj=bundling,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'None',
                'AggregateLowPopulation',
                'Adjacent'])
    if max_bins is not None:
        inputs['MaxBins'] = try_set(
            obj=max_bins,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if sparsify_threshold is not None:
        inputs['SparsifyThreshold'] = try_set(
            obj=sparsify_threshold,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if feature_first_use_penalty is not None:
        inputs['FeatureFirstUsePenalty'] = try_set(
            obj=feature_first_use_penalty,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if feature_reuse_penalty is not None:
        inputs['FeatureReusePenalty'] = try_set(
            obj=feature_reuse_penalty,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if gain_confidence_level is not None:
        inputs['GainConfidenceLevel'] = try_set(
            obj=gain_confidence_level,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if softmax_temperature is not None:
        inputs['SoftmaxTemperature'] = try_set(
            obj=softmax_temperature,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if execution_times is not None:
        inputs['ExecutionTimes'] = try_set(
            obj=execution_times,
            none_acceptable=True,
            is_of_type=bool)
    if feature_fraction is not None:
        inputs['FeatureFraction'] = try_set(
            obj=feature_fraction,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if bagging_size is not None:
        inputs['BaggingSize'] = try_set(
            obj=bagging_size,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if bagging_train_fraction is not None:
        inputs['BaggingTrainFraction'] = try_set(
            obj=bagging_train_fraction,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if split_fraction is not None:
        inputs['SplitFraction'] = try_set(
            obj=split_fraction,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if smoothing is not None:
        inputs['Smoothing'] = try_set(
            obj=smoothing,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if allow_empty_trees is not None:
        inputs['AllowEmptyTrees'] = try_set(
            obj=allow_empty_trees,
            none_acceptable=True,
            is_of_type=bool)
    if feature_compression_level is not None:
        inputs['FeatureCompressionLevel'] = try_set(
            obj=feature_compression_level,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if compress_ensemble is not None:
        inputs['CompressEnsemble'] = try_set(
            obj=compress_ensemble,
            none_acceptable=True,
            is_of_type=bool)
    if max_trees_after_compression is not None:
        inputs['MaxTreesAfterCompression'] = try_set(
            obj=max_trees_after_compression,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if print_test_graph is not None:
        inputs['PrintTestGraph'] = try_set(
            obj=print_test_graph,
            none_acceptable=True,
            is_of_type=bool)
    if print_train_valid_graph is not None:
        inputs['PrintTrainValidGraph'] = try_set(
            obj=print_train_valid_graph,
            none_acceptable=True,
            is_of_type=bool)
    if test_frequency is not None:
        inputs['TestFrequency'] = try_set(
            obj=test_frequency,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if predictor_model is not None:
        outputs['PredictorModel'] = try_set(
            obj=predictor_model, none_acceptable=False, is_of_type=str)

    input_variables = {
        x for x in unlist(inputs.values())
        if isinstance(x, str) and x.startswith("$")}
    output_variables = {
        x for x in unlist(outputs.values())
        if isinstance(x, str) and x.startswith("$")}

    entrypoint = EntryPoint(
        name=entrypoint_name, inputs=inputs, outputs=outputs,
        input_variables=input_variables,
        output_variables=output_variables)
    return entrypoint
