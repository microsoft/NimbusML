# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
Trainers.LightGbmBinaryClassifier
"""

import numbers

from ..utils.entrypoints import EntryPoint
from ..utils.utils import try_set, unlist


def trainers_lightgbmbinaryclassifier(
        training_data,
        predictor_model=None,
        number_of_iterations=100,
        learning_rate=None,
        number_of_leaves=None,
        minimum_example_count_per_leaf=None,
        feature_column_name='Features',
        booster=None,
        label_column_name='Label',
        example_weight_column_name=None,
        row_group_column_name=None,
        normalize_features='Auto',
        caching='Auto',
        unbalanced_sets=False,
        weight_of_positive_examples=1.0,
        sigmoid=0.5,
        evaluation_metric='Logloss',
        maximum_bin_count_per_feature=255,
        verbose=False,
        silent=True,
        number_of_threads=None,
        early_stopping_round=0,
        batch_size=1048576,
        use_categorical_split=None,
        handle_missing_value=True,
        minimum_example_count_per_group=100,
        maximum_categorical_split_point_count=32,
        categorical_smoothing=10.0,
        l2_categorical_regularization=10.0,
        seed=None,
        parallel_trainer=None,
        **params):
    """
    **Description**
        Train a LightGBM binary classification model.

    :param number_of_iterations: Number of iterations. (inputs).
    :param training_data: The data to be used for training (inputs).
    :param learning_rate: Shrinkage rate for trees, used to prevent
        over-fitting. Range: (0,1]. (inputs).
    :param number_of_leaves: Maximum leaves for trees. (inputs).
    :param minimum_example_count_per_leaf: Minimum number of
        instances needed in a child. (inputs).
    :param feature_column_name: Column to use for features (inputs).
    :param booster: Which booster to use, can be gbtree, gblinear or
        dart. gbtree and dart use tree based model while gblinear
        uses linear function. (inputs).
    :param label_column_name: Column to use for labels (inputs).
    :param example_weight_column_name: Column to use for example
        weight (inputs).
    :param row_group_column_name: Column to use for example groupId
        (inputs).
    :param normalize_features: Normalize option for the feature
        column (inputs).
    :param caching: Whether trainer should cache input training data
        (inputs).
    :param unbalanced_sets: Use for binary classification when
        training data is not balanced. (inputs).
    :param weight_of_positive_examples: Control the balance of
        positive and negative weights, useful for unbalanced classes.
        A typical value to consider: sum(negative cases) /
        sum(positive cases). (inputs).
    :param sigmoid: Parameter for the sigmoid function. (inputs).
    :param evaluation_metric: Evaluation metrics. (inputs).
    :param maximum_bin_count_per_feature: Maximum number of bucket
        bin for features. (inputs).
    :param verbose: Verbose (inputs).
    :param silent: Printing running messages. (inputs).
    :param number_of_threads: Number of parallel threads used to run
        LightGBM. (inputs).
    :param early_stopping_round: Rounds of early stopping, 0 will
        disable it. (inputs).
    :param batch_size: Number of entries in a batch when loading
        data. (inputs).
    :param use_categorical_split: Enable categorical split or not.
        (inputs).
    :param handle_missing_value: Enable special handling of missing
        value or not. (inputs).
    :param minimum_example_count_per_group: Minimum number of
        instances per categorical group. (inputs).
    :param maximum_categorical_split_point_count: Max number of
        categorical thresholds. (inputs).
    :param categorical_smoothing: Lapalace smooth term in categorical
        feature spilt. Avoid the bias of small categories. (inputs).
    :param l2_categorical_regularization: L2 Regularization for
        categorical split. (inputs).
    :param seed: Sets the random seed for LightGBM to use. (inputs).
    :param parallel_trainer: Parallel LightGBM Learning Algorithm
        (inputs).
    :param predictor_model: The trained model (outputs).
    """

    entrypoint_name = 'Trainers.LightGbmBinaryClassifier'
    inputs = {}
    outputs = {}

    if number_of_iterations is not None:
        inputs['NumberOfIterations'] = try_set(
            obj=number_of_iterations,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if training_data is not None:
        inputs['TrainingData'] = try_set(
            obj=training_data,
            none_acceptable=False,
            is_of_type=str)
    if learning_rate is not None:
        inputs['LearningRate'] = try_set(
            obj=learning_rate,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if number_of_leaves is not None:
        inputs['NumberOfLeaves'] = try_set(
            obj=number_of_leaves,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if minimum_example_count_per_leaf is not None:
        inputs['MinimumExampleCountPerLeaf'] = try_set(
            obj=minimum_example_count_per_leaf,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if feature_column_name is not None:
        inputs['FeatureColumnName'] = try_set(
            obj=feature_column_name,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if booster is not None:
        inputs['Booster'] = try_set(
            obj=booster,
            none_acceptable=True,
            is_of_type=dict)
    if label_column_name is not None:
        inputs['LabelColumnName'] = try_set(
            obj=label_column_name,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if example_weight_column_name is not None:
        inputs['ExampleWeightColumnName'] = try_set(
            obj=example_weight_column_name,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if row_group_column_name is not None:
        inputs['RowGroupColumnName'] = try_set(
            obj=row_group_column_name,
            none_acceptable=True,
            is_of_type=str,
            is_column=True)
    if normalize_features is not None:
        inputs['NormalizeFeatures'] = try_set(
            obj=normalize_features,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'No',
                'Warn',
                'Auto',
                'Yes'])
    if caching is not None:
        inputs['Caching'] = try_set(
            obj=caching,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'Auto',
                'Memory',
                'None'])
    if unbalanced_sets is not None:
        inputs['UnbalancedSets'] = try_set(
            obj=unbalanced_sets,
            none_acceptable=True,
            is_of_type=bool)
    if weight_of_positive_examples is not None:
        inputs['WeightOfPositiveExamples'] = try_set(
            obj=weight_of_positive_examples,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if sigmoid is not None:
        inputs['Sigmoid'] = try_set(
            obj=sigmoid,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if evaluation_metric is not None:
        inputs['EvaluationMetric'] = try_set(
            obj=evaluation_metric,
            none_acceptable=True,
            is_of_type=str,
            values=[
                'None',
                'Default',
                'Logloss',
                'Error',
                'AreaUnderCurve'])
    if maximum_bin_count_per_feature is not None:
        inputs['MaximumBinCountPerFeature'] = try_set(
            obj=maximum_bin_count_per_feature,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if verbose is not None:
        inputs['Verbose'] = try_set(
            obj=verbose,
            none_acceptable=True,
            is_of_type=bool)
    if silent is not None:
        inputs['Silent'] = try_set(
            obj=silent,
            none_acceptable=True,
            is_of_type=bool)
    if number_of_threads is not None:
        inputs['NumberOfThreads'] = try_set(
            obj=number_of_threads,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if early_stopping_round is not None:
        inputs['EarlyStoppingRound'] = try_set(
            obj=early_stopping_round,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if batch_size is not None:
        inputs['BatchSize'] = try_set(
            obj=batch_size,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if use_categorical_split is not None:
        inputs['UseCategoricalSplit'] = try_set(
            obj=use_categorical_split, none_acceptable=True, is_of_type=bool)
    if handle_missing_value is not None:
        inputs['HandleMissingValue'] = try_set(
            obj=handle_missing_value,
            none_acceptable=True,
            is_of_type=bool)
    if minimum_example_count_per_group is not None:
        inputs['MinimumExampleCountPerGroup'] = try_set(
            obj=minimum_example_count_per_group,
            none_acceptable=True,
            is_of_type=numbers.Real,
            valid_range={
                'Inf': 0,
                'Max': 2147483647})
    if maximum_categorical_split_point_count is not None:
        inputs['MaximumCategoricalSplitPointCount'] = try_set(
            obj=maximum_categorical_split_point_count,
            none_acceptable=True,
            is_of_type=numbers.Real,
            valid_range={
                'Inf': 0,
                'Max': 2147483647})
    if categorical_smoothing is not None:
        inputs['CategoricalSmoothing'] = try_set(
            obj=categorical_smoothing,
            none_acceptable=True,
            is_of_type=numbers.Real, valid_range={'Min': 0.0})
    if l2_categorical_regularization is not None:
        inputs['L2CategoricalRegularization'] = try_set(
            obj=l2_categorical_regularization,
            none_acceptable=True,
            is_of_type=numbers.Real, valid_range={'Min': 0.0})
    if seed is not None:
        inputs['Seed'] = try_set(
            obj=seed,
            none_acceptable=True,
            is_of_type=numbers.Real)
    if parallel_trainer is not None:
        inputs['ParallelTrainer'] = try_set(
            obj=parallel_trainer,
            none_acceptable=True,
            is_of_type=dict)
    if predictor_model is not None:
        outputs['PredictorModel'] = try_set(
            obj=predictor_model, none_acceptable=False, is_of_type=str)

    input_variables = {
        x for x in unlist(inputs.values())
        if isinstance(x, str) and x.startswith("$")}
    output_variables = {
        x for x in unlist(outputs.values())
        if isinstance(x, str) and x.startswith("$")}

    entrypoint = EntryPoint(
        name=entrypoint_name, inputs=inputs, outputs=outputs,
        input_variables=input_variables,
        output_variables=output_variables)
    return entrypoint
