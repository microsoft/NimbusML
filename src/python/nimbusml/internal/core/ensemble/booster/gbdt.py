# --------------------------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------------------------
# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
Gbdt
"""

__all__ = ["Gbdt"]

import numbers

from ....utils.entrypoints import Component
from ....utils.utils import trace, try_set


class Gbdt(Component):
    """

    Traditional Gradient Boosting Decision Tree.

    :param unbalanced_sets: Use for binary classification when classes are not
        balanced.

    :param min_split_gain: Minimum loss reduction required to make a further
        partition on a leaf node of the tree. the larger, the more conservative
        the algorithm will be.

    :param max_depth: Maximum depth of a tree. 0 means no limit. However, tree
        still grows by best-first.

    :param min_child_weight: Minimum sum of instance weight(hessian) needed in
        a child. If the tree partition step results in a leaf node with the sum
        of instance weight less than min_child_weight, then the building
        process will give up further partitioning. In linear regression mode,
        this simply corresponds to minimum number of instances needed to be in
        each node. The larger, the more conservative the algorithm will be.

    :param subsample_freq: Subsample frequency. 0 means no subsample. If
        subsampleFreq > 0, it will use a subset(ratio=subsample) to train. And
        the subset will be updated on every Subsample iteratinos.

    :param subsample: Subsample ratio of the training instance. Setting it to
        0.5 means that LightGBM randomly collected half of the data instances
        to grow trees and this will prevent overfitting. Range: (0,1].

    :param feature_fraction: Subsample ratio of columns when constructing each
        tree. Range: (0,1].

    :param reg_lambda: L2 regularization term on weights, increasing this value
        will make model more conservative.

    :param reg_alpha: L1 regularization term on weights, increase this value
        will make model more conservative.

    :param scale_pos_weight: Control the balance of positive and negative
        weights, useful for unbalanced classes. A typical value to consider:
        sum(negative cases) / sum(positive cases).

    :param params: Additional arguments sent to compute engine.

    .. seealso::
        :py:func:`Dart <nimbusml.ensemble.booster.Dart>`,
        :py:func:`Goss <nimbusml.ensemble.booster.Goss>`,
        :py:func:`LightGbmBinaryClassifier
        <nimbusml.ensemble.LightGbmBinaryClassifier>`,
        :py:func:`LightGbmClassifier <nimbusml.ensemble.LightGbmClassifier>`,
        :py:func:`LightGbmRanker <nimbusml.ensemble.LightGbmRanker>`,
        :py:func:`LightGbmRegressor <nimbusml.ensemble.LightGbmRegressor>`

    .. index:: ensemble, booster

    Example:
       .. literalinclude:: /../nimbusml/examples/LightGbmRegressor.py
              :language: python
    """

    @trace
    def __init__(
            self,
            unbalanced_sets=False,
            min_split_gain=0.0,
            max_depth=0,
            min_child_weight=0.1,
            subsample_freq=0,
            subsample=1.0,
            feature_fraction=1.0,
            reg_lambda=0.01,
            reg_alpha=0.0,
            scale_pos_weight=1.0,
            **params):

        self.unbalanced_sets = unbalanced_sets
        self.min_split_gain = min_split_gain
        self.max_depth = max_depth
        self.min_child_weight = min_child_weight
        self.subsample_freq = subsample_freq
        self.subsample = subsample
        self.feature_fraction = feature_fraction
        self.reg_lambda = reg_lambda
        self.reg_alpha = reg_alpha
        self.scale_pos_weight = scale_pos_weight
        self.kind = 'BoosterParameterFunction'
        self.name = 'gbdt'
        self.settings = {}

        if unbalanced_sets is not None:
            self.settings['UnbalancedSets'] = try_set(
                obj=unbalanced_sets, none_acceptable=True, is_of_type=bool)
        if min_split_gain is not None:
            self.settings['MinSplitGain'] = try_set(
                obj=min_split_gain,
                none_acceptable=True,
                is_of_type=numbers.Real, valid_range={'Min': 0.0})
        if max_depth is not None:
            self.settings['MaxDepth'] = try_set(
                obj=max_depth,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Max': 2147483647,
                    'Min': 0})
        if min_child_weight is not None:
            self.settings['MinChildWeight'] = try_set(
                obj=min_child_weight,
                none_acceptable=True,
                is_of_type=numbers.Real, valid_range={'Min': 0.0})
        if subsample_freq is not None:
            self.settings['SubsampleFreq'] = try_set(
                obj=subsample_freq,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Max': 2147483647,
                    'Min': 0})
        if subsample is not None:
            self.settings['Subsample'] = try_set(
                obj=subsample,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Inf': 0.0,
                    'Max': 1.0})
        if feature_fraction is not None:
            self.settings['FeatureFraction'] = try_set(
                obj=feature_fraction,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Inf': 0.0,
                    'Max': 1.0})
        if reg_lambda is not None:
            self.settings['RegLambda'] = try_set(
                obj=reg_lambda,
                none_acceptable=True,
                is_of_type=numbers.Real, valid_range={'Min': 0.0})
        if reg_alpha is not None:
            self.settings['RegAlpha'] = try_set(
                obj=reg_alpha,
                none_acceptable=True,
                is_of_type=numbers.Real, valid_range={'Min': 0.0})
        if scale_pos_weight is not None:
            self.settings['ScalePosWeight'] = try_set(
                obj=scale_pos_weight,
                none_acceptable=True,
                is_of_type=numbers.Real)

        super(
            Gbdt,
            self).__init__(
            name=self.name,
            settings=self.settings,
            kind=self.kind)
