# --------------------------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------------------------
# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
Dart
"""

__all__ = ["Dart"]

import numbers

from ....utils.entrypoints import Component
from ....utils.utils import trace, try_set


class Dart(Component):
    """

    Dropouts meet Multiple Additive Regresion Trees.

    .. remarks::
        `Multiple Additive Regression Trees (MART)
        <https://arxiv.org/abs/1505.01866>`_ is an
        ensemble method of boosted regression trees. The Dropouts meet
        Multiple Additive Regression
    Trees (DART) employs dropouts in MART and overcomes the issues of over-
        specialization of MART,
    achiving better performance in many tasks.


        **Reference**

            `https://arxiv.org/abs/1505.01866
            <https://arxiv.org/abs/1505.01866>`_


    :param drop_rate: Drop ratio for trees. Range:(0,1).

    :param max_drop: Max number of dropped tree in a boosting round.

    :param skip_drop: Probability for not perform dropping in a boosting round.

    :param xgboost_dart_mode: True will enable xgboost dart mode.

    :param uniform_drop: True will enable uniform drop.

    :param unbalanced_sets: Use for binary classification when classes are not
        balanced.

    :param min_split_gain: Minimum loss reduction required to make a further
        partition on a leaf node of the tree. the larger, the more conservative
        the algorithm will be.

    :param max_depth: Maximum depth of a tree. 0 means no limit. However, tree
        still grows by best-first.

    :param min_child_weight: Minimum sum of instance weight(hessian) needed in
        a child. If the tree partition step results in a leaf node with the sum
        of instance weight less than min_child_weight, then the building
        process will give up further partitioning. In linear regression mode,
        this simply corresponds to minimum number of instances needed to be in
        each node. The larger, the more conservative the algorithm will be.

    :param subsample_freq: Subsample frequency. 0 means no subsample. If
        subsampleFreq > 0, it will use a subset(ratio=subsample) to train. And
        the subset will be updated on every Subsample iteratinos.

    :param subsample: Subsample ratio of the training instance. Setting it to
        0.5 means that LightGBM randomly collected half of the data instances
        to grow trees and this will prevent overfitting. Range: (0,1].

    :param feature_fraction: Subsample ratio of columns when constructing each
        tree. Range: (0,1].

    :param reg_lambda: L2 regularization term on weights, increasing this value
        will make model more conservative.

    :param reg_alpha: L1 regularization term on weights, increase this value
        will make model more conservative.

    :param scale_pos_weight: Control the balance of positive and negative
        weights, useful for unbalanced classes. A typical value to consider:
        sum(negative cases) / sum(positive cases).

    :param params: Additional arguments sent to compute engine.

    .. seealso::
        :py:func:`Gbdt <nimbusml.ensemble.booster.Gbdt>`,
        :py:func:`Goss <nimbusml.ensemble.booster.Goss>`,
        :py:func:`LightGbmBinaryClassifier
        <nimbusml.ensemble.LightGbmBinaryClassifier>`,
        :py:func:`LightGbmClassifier <nimbusml.ensemble.LightGbmClassifier>`,
        :py:func:`LightGbmRanker <nimbusml.ensemble.LightGbmRanker>`,
        :py:func:`LightGbmRegressor <nimbusml.ensemble.LightGbmRegressor>`

    .. index:: ensemble, booster

    Example:
       .. literalinclude:: /../nimbusml/examples/LightGbmClassifier.py
              :language: python
    """

    @trace
    def __init__(
            self,
            drop_rate=0.1,
            max_drop=1,
            skip_drop=0.5,
            xgboost_dart_mode=False,
            uniform_drop=False,
            unbalanced_sets=False,
            min_split_gain=0.0,
            max_depth=0,
            min_child_weight=0.1,
            subsample_freq=0,
            subsample=1.0,
            feature_fraction=1.0,
            reg_lambda=0.01,
            reg_alpha=0.0,
            scale_pos_weight=1.0,
            **params):

        self.drop_rate = drop_rate
        self.max_drop = max_drop
        self.skip_drop = skip_drop
        self.xgboost_dart_mode = xgboost_dart_mode
        self.uniform_drop = uniform_drop
        self.unbalanced_sets = unbalanced_sets
        self.min_split_gain = min_split_gain
        self.max_depth = max_depth
        self.min_child_weight = min_child_weight
        self.subsample_freq = subsample_freq
        self.subsample = subsample
        self.feature_fraction = feature_fraction
        self.reg_lambda = reg_lambda
        self.reg_alpha = reg_alpha
        self.scale_pos_weight = scale_pos_weight
        self.kind = 'BoosterParameterFunction'
        self.name = 'dart'
        self.settings = {}

        if drop_rate is not None:
            self.settings['DropRate'] = try_set(
                obj=drop_rate,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Inf': 0.0,
                    'Max': 1.0})
        if max_drop is not None:
            self.settings['MaxDrop'] = try_set(
                obj=max_drop,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Inf': 0,
                    'Max': 2147483647})
        if skip_drop is not None:
            self.settings['SkipDrop'] = try_set(
                obj=skip_drop,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Inf': 0.0,
                    'Max': 1.0})
        if xgboost_dart_mode is not None:
            self.settings['XgboostDartMode'] = try_set(
                obj=xgboost_dart_mode, none_acceptable=True, is_of_type=bool)
        if uniform_drop is not None:
            self.settings['UniformDrop'] = try_set(
                obj=uniform_drop, none_acceptable=True, is_of_type=bool)
        if unbalanced_sets is not None:
            self.settings['UnbalancedSets'] = try_set(
                obj=unbalanced_sets, none_acceptable=True, is_of_type=bool)
        if min_split_gain is not None:
            self.settings['MinSplitGain'] = try_set(
                obj=min_split_gain,
                none_acceptable=True,
                is_of_type=numbers.Real, valid_range={'Min': 0.0})
        if max_depth is not None:
            self.settings['MaxDepth'] = try_set(
                obj=max_depth,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Max': 2147483647,
                    'Min': 0})
        if min_child_weight is not None:
            self.settings['MinChildWeight'] = try_set(
                obj=min_child_weight,
                none_acceptable=True,
                is_of_type=numbers.Real, valid_range={'Min': 0.0})
        if subsample_freq is not None:
            self.settings['SubsampleFreq'] = try_set(
                obj=subsample_freq,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Max': 2147483647,
                    'Min': 0})
        if subsample is not None:
            self.settings['Subsample'] = try_set(
                obj=subsample,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Inf': 0.0,
                    'Max': 1.0})
        if feature_fraction is not None:
            self.settings['FeatureFraction'] = try_set(
                obj=feature_fraction,
                none_acceptable=True,
                is_of_type=numbers.Real,
                valid_range={
                    'Inf': 0.0,
                    'Max': 1.0})
        if reg_lambda is not None:
            self.settings['RegLambda'] = try_set(
                obj=reg_lambda,
                none_acceptable=True,
                is_of_type=numbers.Real, valid_range={'Min': 0.0})
        if reg_alpha is not None:
            self.settings['RegAlpha'] = try_set(
                obj=reg_alpha,
                none_acceptable=True,
                is_of_type=numbers.Real, valid_range={'Min': 0.0})
        if scale_pos_weight is not None:
            self.settings['ScalePosWeight'] = try_set(
                obj=scale_pos_weight,
                none_acceptable=True,
                is_of_type=numbers.Real)

        super(
            Dart,
            self).__init__(
            name=self.name,
            settings=self.settings,
            kind=self.kind)
