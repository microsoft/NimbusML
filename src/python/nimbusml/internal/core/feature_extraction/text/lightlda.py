# --------------------------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------------------------
# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
LightLda
"""

__all__ = ["LightLda"]


from ....entrypoints.transforms_lightlda import transforms_lightlda
from ....utils.utils import trace
from ...base_pipeline_item import BasePipelineItem, DefaultSignature


class LightLda(BasePipelineItem, DefaultSignature):
    """

    The LDA transform implements LightLDA, a state-of-the-art
    implementation of Latent Dirichlet Allocation.

    .. remarks::
        Latent Dirichlet Allocation is a well-known topic modeling algorithm
        that infers topical structure from text
        data, and can be used to featurize any text fields as low-dimensional
        topical vectors. LightLDA is an extremely
        efficient implementation of LDA developed in MSR-Asia that
        incorporates a number of optimization techniques
        `(http://arxiv.org/abs/1412.1576) <http://arxiv.org/abs/1412.1576>`_.
        With the LDA transform, we can
        train a topic model to produce 1 million topics with 1 million
        vocabulary on a 1-billion-token document set one
        a single machine in a few hours (typically, LDA at this scale takes
        days and requires large clusters). The most
        significant innovation is a super-efficient O(1) Metropolis-Hastings
        sampling algorithm, whose running cost is
        (surprisingly) agnostic of model size, allowing it to converges
        nearly an order of magnitude faster than other
        Gibbs samplers.


    :param num_topic: The number of topics.

    :param number_of_threads: The number of training threads. Default value
        depends on number of logical processors.

    :param num_max_doc_token: The threshold of maximum count of tokens per doc.

    :param alpha_sum: Dirichlet prior on document-topic vectors.

    :param beta: Dirichlet prior on vocab-topic vectors.

    :param mhstep: Number of Metropolis Hasting step.

    :param num_iterations: Number of iterations.

    :param likelihood_interval: Compute log likelihood over local dataset on
        this iteration interval.

    :param num_summary_term_per_topic: The number of words to summarize the
        topic.

    :param num_burnin_iterations: The number of burn-in iterations.

    :param reset_random_generator: Reset the random number generator for each
        document.

    :param output_topic_word_summary: Whether to output the topic-word summary
        in text format.

    :param params: Additional arguments sent to compute engine.

    .. seealso::
        :py:class:`NGramFeaturizer
        <nimbusml.feature_extraction.text.NGramFeaturizer>`,
        :py:class:`Sentiment
        <nimbusml.feature_extraction.text.Sentiment>`,
        :py:class:`WordEmbedding
        <nimbusml.feature_extraction.text.WordEmbedding>`.

    .. index:: transform, featurizer, text

    Example:
       .. literalinclude:: /../nimbusml/examples/LightLda.py
              :language: python
    """

    @trace
    def __init__(
            self,
            num_topic=100,
            number_of_threads=0,
            num_max_doc_token=512,
            alpha_sum=100.0,
            beta=0.01,
            mhstep=4,
            num_iterations=200,
            likelihood_interval=5,
            num_summary_term_per_topic=10,
            num_burnin_iterations=10,
            reset_random_generator=False,
            output_topic_word_summary=False,
            **params):
        BasePipelineItem.__init__(
            self, type='transform', **params)

        self.num_topic = num_topic
        self.number_of_threads = number_of_threads
        self.num_max_doc_token = num_max_doc_token
        self.alpha_sum = alpha_sum
        self.beta = beta
        self.mhstep = mhstep
        self.num_iterations = num_iterations
        self.likelihood_interval = likelihood_interval
        self.num_summary_term_per_topic = num_summary_term_per_topic
        self.num_burnin_iterations = num_burnin_iterations
        self.reset_random_generator = reset_random_generator
        self.output_topic_word_summary = output_topic_word_summary

    @property
    def _entrypoint(self):
        return transforms_lightlda

    @trace
    def _get_node(self, **all_args):

        input_columns = self.input
        if input_columns is None and 'input' in all_args:
            input_columns = all_args['input']
        if 'input' in all_args:
            all_args.pop('input')

        output_columns = self.output
        if output_columns is None and 'output' in all_args:
            output_columns = all_args['output']
        if 'output' in all_args:
            all_args.pop('output')

        # validate input
        if input_columns is None:
            raise ValueError(
                "'None' input passed when it cannot be none.")

        if not isinstance(input_columns, list):
            raise ValueError(
                "input has to be a list of strings, instead got %s" %
                type(input_columns))

        # validate output
        if output_columns is None:
            output_columns = input_columns

        if not isinstance(output_columns, list):
            raise ValueError(
                "output has to be a list of strings, instead got %s" %
                type(output_columns))

        algo_args = dict(
            column=[
                dict(
                    Source=i,
                    Name=o) for i,
                o in zip(
                    input_columns,
                    output_columns)] if input_columns else None,
            num_topic=self.num_topic,
            num_threads=self.number_of_threads,
            num_max_doc_token=self.num_max_doc_token,
            alpha_sum=self.alpha_sum,
            beta=self.beta,
            mhstep=self.mhstep,
            num_iterations=self.num_iterations,
            likelihood_interval=self.likelihood_interval,
            num_summary_term_per_topic=self.num_summary_term_per_topic,
            num_burnin_iterations=self.num_burnin_iterations,
            reset_random_generator=self.reset_random_generator,
            output_topic_word_summary=self.output_topic_word_summary)

        all_args.update(algo_args)
        return self._entrypoint(**all_args)
