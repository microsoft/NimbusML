# --------------------------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------------------------
# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
NgramHash
"""

__all__ = ["NgramHash"]

import numbers

from .....utils.entrypoints import Component
from .....utils.utils import trace, try_set


class NgramHash(Component):
    """

    Extracts NGrams from text and convert them to vector using hashing
    trick.

    .. remarks::
        The ``NGramFeaturizer`` transform produces a bag of counts of
        sequences of consecutive words, called n-grams, from a given corpus
        of text.
        There are two ways it can do this:

        * build a dictionary of n-grams and use the id in the dictionary as
        the index in the bag;
        * hash each n-gram and use the hash value as the index in the bag.

        This class provide the text extractor that implement the second. In
        :py:class:`NGramFeaturizer
        <nimbusml.feature_extraction.text.NGramFeaturizer>`,
        users should specify which text extractor to use as the argument.

        The purpose of hashing is to convert variable-length text documents
        into equal-length numeric feature vectors, to support
        dimensionality reduction and to make the lookup of feature weights
        faster.

        The n-grams are represented as count vectors, with vector slots
        corresponding to their hashes. Embedding ngrams in
        a vector space allows their contents to be compared in an efficient
        manner.
        The slot values in the vector can be weighted by the following
        factors:

        * *term frequency* - The number of occurrences of the slot in the
        text
        * *inverse document frequency* - A ratio (the logarithm of
          inverse relative slot frequency) that measures the information a
        slot
          provides by determining how common or rare it is across the entire
        text.
        * *term frequency-inverse document frequency* - the product
          term frequency and the inverse document frequency.

    :param number_of_bits: Number of bits to hash into. Must be between 1 and
        30, inclusive.

    :param ngram_length: Ngram length.

    :param skip_length: Maximum number of tokens to skip when constructing an
        n-gram.

    :param all_lengths: Whether to include all n-gram lengths up to ngramLength
        or only ngramLength.

    :param seed: Hashing seed.

    :param ordered: Whether the position of each source column should be
        included in the hash (when there are multiple source columns).

    :param maximum_number_of_inverts: Limit the number of keys used to generate
        the slot name to this many. 0 means no invert hashing, -1 means no
        limit.

    :param params: Additional arguments sent to compute engine.

    .. seealso::
        :py:class:`NGramFeaturizer
        <nimbusml.feature_extraction.text.NGramFeaturizer>`,
        :py:class:`Ngram <nimbusml.feature_extraction.text.extractor.Ngram>`

    .. index:: transform, featurizer, text

    Example:
       .. literalinclude:: /../nimbusml/examples/NGramFeaturizer3.py
              :language: python
    """

    @trace
    def __init__(
            self,
            number_of_bits=16,
            ngram_length=1,
            skip_length=0,
            all_lengths=True,
            seed=314489979,
            ordered=True,
            maximum_number_of_inverts=0,
            **params):

        self.number_of_bits = number_of_bits
        self.ngram_length = ngram_length
        self.skip_length = skip_length
        self.all_lengths = all_lengths
        self.seed = seed
        self.ordered = ordered
        self.maximum_number_of_inverts = maximum_number_of_inverts
        self.kind = 'NgramExtractor'
        self.name = 'NGramHash'
        self.settings = {}

        if number_of_bits is not None:
            self.settings['NumberOfBits'] = try_set(
                obj=number_of_bits,
                none_acceptable=True,
                is_of_type=numbers.Real)
        if ngram_length is not None:
            self.settings['NgramLength'] = try_set(
                obj=ngram_length,
                none_acceptable=True,
                is_of_type=numbers.Real)
        if skip_length is not None:
            self.settings['SkipLength'] = try_set(
                obj=skip_length,
                none_acceptable=True,
                is_of_type=numbers.Real)
        if all_lengths is not None:
            self.settings['AllLengths'] = try_set(
                obj=all_lengths, none_acceptable=True, is_of_type=bool)
        if seed is not None:
            self.settings['Seed'] = try_set(
                obj=seed,
                none_acceptable=True,
                is_of_type=numbers.Real)
        if ordered is not None:
            self.settings['Ordered'] = try_set(
                obj=ordered, none_acceptable=True, is_of_type=bool)
        if maximum_number_of_inverts is not None:
            self.settings['MaximumNumberOfInverts'] = try_set(
                obj=maximum_number_of_inverts,
                none_acceptable=True,
                is_of_type=numbers.Real)

        super(
            NgramHash,
            self).__init__(
            name=self.name,
            settings=self.settings,
            kind=self.kind)
