# --------------------------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------------------------
# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
FactorizationMachineBinaryClassifier
"""

__all__ = ["FactorizationMachineBinaryClassifier"]


from ...entrypoints.trainers_fieldawarefactorizationmachinebinaryclassifier import \
    trainers_fieldawarefactorizationmachinebinaryclassifier
from ...utils.utils import trace
from ..base_pipeline_item import BasePipelineItem, DefaultSignatureWithRoles


class FactorizationMachineBinaryClassifier(
        BasePipelineItem, DefaultSignatureWithRoles):
    """

    Train a field-aware factorization machine for binary classification.

    .. remarks::
        Field Aware Factorization Machines use, in addition to the input
        variables, factorized
        parameters to model the interaction between pairs of variables. The
        algorithm is
        particularly useful for high dimensional datasets which can be very
        sparse (e.g.
        click-prediction for advertising systems). An advantage of FFM over
        SVMs is that the
        training data does not need to be stored in memory, and the
        coefficients can be optimized
        directly.



        **Reference**

            `Field Aware Factorization Machines
            <https://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf>`_,
            `Field-aware Factorization Machines for CTR Prediction
            <https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf>`_,
            `Adaptive Subgradient Methods for Online Learning and Stochastic
            Optimization
            <https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf>`_


    :param learning_rate: Initial learning rate.

    :param iters: Number of training iterations.

    :param latent_dim: Latent space dimension.

    :param lambda_linear: Regularization coefficient of linear weights.

    :param lambda_latent: Regularization coefficient of latent weights.

    :param normalize: Specifies the type of automatic normalization used:

        * ``"Auto"``: if normalization is needed, it is performed
          automatically. This is the default choice.
        * ``"No"``: no normalization is performed.
        * ``"Yes"``: normalization is performed.
        * ``"Warn"``: if normalization is needed, a warning
          message is displayed, but normalization is not performed.

        Normalization rescales disparate data ranges to a standard scale.
        Feature
        scaling insures the distances between data points are proportional
        and
        enables various optimization methods such as gradient descent to
        converge
        much faster. If normalization is performed, a ``MaxMin`` normalizer
        is
        used. It normalizes values in an interval [a, b] where ``-1 <= a <=
        0``
        and ``0 <= b <= 1`` and ``b - a = 1``. This normalizer preserves
        sparsity by mapping zero to zero.

    :param norm: Whether to normalize the input vectors so that the
        concatenation of all fields' feature vectors is unit-length.

    :param caching: Whether learner should cache input training data.

    :param shuffle: Whether to shuffle for each training iteration.

    :param verbose: Report traning progress or not.

    :param radius: Radius of initial latent factors.

    :param params: Additional arguments sent to compute engine.

    .. seealso::
        :py:func:`LogisticRegressionClassifier
        <nimbusml.linear_model.LogisticRegressionClassifier>`,
        :py:func:`AveragedPerceptronBinaryClassifier
        <nimbusml.linear_model.AveragedPerceptronBinaryClassifier>`,
        :py:func:`GamBinaryClassifier <nimbusml.ensemble.GamBinaryClassifier>`,
        :py:func:`SgdBinaryClassifier
        <nimbusml.linear_model.SgdBinaryClassifier>`

    .. index:: models, stochastic, online, gradient, descent

    Example:
      .. literalinclude:: /../nimbusml/examples/FactorizationMachineBinaryClassifier.py
             :language: python
    """

    @trace
    def __init__(
            self,
            learning_rate=0.1,
            iters=5,
            latent_dim=20,
            lambda_linear=0.0001,
            lambda_latent=0.0001,
            normalize='Auto',
            norm=True,
            caching='Auto',
            shuffle=True,
            verbose=True,
            radius=0.5,
            **params):
        BasePipelineItem.__init__(
            self, type='classifier', **params)

        self.learning_rate = learning_rate
        self.iters = iters
        self.latent_dim = latent_dim
        self.lambda_linear = lambda_linear
        self.lambda_latent = lambda_latent
        self.normalize = normalize
        self.norm = norm
        self.caching = caching
        self.shuffle = shuffle
        self.verbose = verbose
        self.radius = radius

    @property
    def _entrypoint(self):
        return trainers_fieldawarefactorizationmachinebinaryclassifier

    @trace
    def _get_node(self, **all_args):
        algo_args = dict(
            feature_column=self._getattr_role(
                'feature_column',
                all_args),
            label_column=self._getattr_role(
                'label_column',
                all_args),
            learning_rate=self.learning_rate,
            iters=self.iters,
            latent_dim=self.latent_dim,
            lambda_linear=self.lambda_linear,
            lambda_latent=self.lambda_latent,
            normalize_features=self.normalize,
            norm=self.norm,
            caching=self.caching,
            shuffle=self.shuffle,
            verbose=self.verbose,
            radius=self.radius)

        all_args.update(algo_args)
        return self._entrypoint(**all_args)
