# --------------------------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------------------------
# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
NaiveBayesClassifier
"""

__all__ = ["NaiveBayesClassifier"]


from sklearn.base import ClassifierMixin

from ..base_predictor import BasePredictor
from ..internal.core.naive_bayes.naivebayesclassifier import \
    NaiveBayesClassifier as core
from ..internal.utils.utils import trace


class NaiveBayesClassifier(core, BasePredictor, ClassifierMixin):
    """

    Machine Learning Naive Bayes Classifier

    .. remarks::
        `Naive Bayes <https://en.wikipedia.org/wiki/Naive_Bayes_classifier>`_
        is a probabilistic classifier that can be used for multiclass
        problems. Using `Bayes' theorem
        <https://en.wikipedia.org/wiki/Bayes%27_theorem>`_, the conditional
        probability for
        a sample belonging to a class can be calculated based on the sample
        count for each feature combination groups.
        However, Naive Bayes Classifier is feasible only if the number of
        features and the values each feature can take is relatively
        small. It also assumes that the features are strictly independent.


        **Reference**

            `Naive Bayes <https://en.wikipedia.org/wiki/Naive_Bayes_classifier>`_


    :param feature: Column to use for features.

    :param label: Column to use for labels.

    :param normalize: Specifies the type of automatic normalization used:

        * ``"Auto"``: if normalization is needed, it is performed
          automatically. This is the default choice.
        * ``"No"``: no normalization is performed.
        * ``"Yes"``: normalization is performed.
        * ``"Warn"``: if normalization is needed, a warning
          message is displayed, but normalization is not performed.

        Normalization rescales disparate data ranges to a standard scale.
        Feature
        scaling insures the distances between data points are proportional
        and
        enables various optimization methods such as gradient descent to
        converge
        much faster. If normalization is performed, a ``MaxMin`` normalizer
        is
        used. It normalizes values in an interval [a, b] where ``-1 <= a <=
        0``
        and ``0 <= b <= 1`` and ``b - a = 1``. This normalizer preserves
        sparsity by mapping zero to zero.

    :param caching: Whether trainer should cache input training data.

    :param params: Additional arguments sent to compute engine.

    .. seealso::
        :py:func:`LogisticRegressionClassifier
        <nimbusml.linear_model.LogisticRegressionClassifier>`,
        :py:func:`FastLinearClassifier
        <nimbusml.linear_model.FastLinearClassifier>`,
        :py:func:`LightGbmClassifier <nimbusml.ensemble.LightGbmClassifier>`.

    .. index:: models, classification, naive

    Example:
       .. literalinclude:: /../nimbusml/examples/NaiveBayesClassifier.py
              :language: python
    """

    @trace
    def __init__(
            self,
            feature='Features',
            label='Label',
            normalize='Auto',
            caching='Auto',
            **params):

        BasePredictor.__init__(self, type='classifier', **params)
        core.__init__(
            self,
            feature=feature,
            label=label,
            normalize=normalize,
            caching=caching,
            **params)

    @trace
    def decision_function(self, X, **params):
        '''
        Returns score values
        '''
        return self._decision_function(X, **params)

    def get_params(self, deep=False):
        """
        Get the parameters for this operator.
        """
        return core.get_params(self)
