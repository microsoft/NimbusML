# --------------------------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------------------------
# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
Dart
"""

__all__ = ["Dart"]


from ...internal.core.ensemble.booster.dart import Dart as core
from ...internal.utils.utils import trace


class Dart(core):
    """

    Dropouts meet Multiple Additive Regresion Trees.

    .. remarks::
        `Multiple Additive Regression Trees (MART)
        <https://arxiv.org/abs/1505.01866>`_ is an
        ensemble method of boosted regression trees. The Dropouts meet
        Multiple Additive Regression
    Trees (DART) employs dropouts in MART and overcomes the issues of over-
        specialization of MART,
    achiving better performance in many tasks.


        **Reference**

            `https://arxiv.org/abs/1505.01866
            <https://arxiv.org/abs/1505.01866>`_


    :param drop_rate: Drop ratio for trees. Range:(0,1).

    :param max_drop: Max number of dropped tree in a boosting round.

    :param skip_drop: Probability for not perform dropping in a boosting round.

    :param xgboost_dart_mode: True will enable xgboost dart mode.

    :param uniform_drop: True will enable uniform drop.

    :param unbalanced_sets: Use for binary classification when classes are not
        balanced.

    :param min_split_gain: Minimum loss reduction required to make a further
        partition on a leaf node of the tree. the larger, the more conservative
        the algorithm will be.

    :param max_depth: Maximum depth of a tree. 0 means no limit. However, tree
        still grows by best-first.

    :param min_child_weight: Minimum sum of instance weight(hessian) needed in
        a child. If the tree partition step results in a leaf node with the sum
        of instance weight less than min_child_weight, then the building
        process will give up further partitioning. In linear regression mode,
        this simply corresponds to minimum number of instances needed to be in
        each node. The larger, the more conservative the algorithm will be.

    :param subsample_freq: Subsample frequency. 0 means no subsample. If
        subsampleFreq > 0, it will use a subset(ratio=subsample) to train. And
        the subset will be updated on every Subsample iteratinos.

    :param subsample: Subsample ratio of the training instance. Setting it to
        0.5 means that LightGBM randomly collected half of the data instances
        to grow trees and this will prevent overfitting. Range: (0,1].

    :param feature_fraction: Subsample ratio of columns when constructing each
        tree. Range: (0,1].

    :param reg_lambda: L2 regularization term on weights, increasing this value
        will make model more conservative.

    :param reg_alpha: L1 regularization term on weights, increase this value
        will make model more conservative.

    :param scale_pos_weight: Control the balance of positive and negative
        weights, useful for unbalanced classes. A typical value to consider:
        sum(negative cases) / sum(positive cases).

    :param params: Additional arguments sent to compute engine.

    .. seealso::
        :py:func:`Gbdt <nimbusml.ensemble.booster.Gbdt>`,
        :py:func:`Goss <nimbusml.ensemble.booster.Goss>`,
        :py:func:`LightGbmBinaryClassifier
        <nimbusml.ensemble.LightGbmBinaryClassifier>`,
        :py:func:`LightGbmClassifier <nimbusml.ensemble.LightGbmClassifier>`,
        :py:func:`LightGbmRanker <nimbusml.ensemble.LightGbmRanker>`,
        :py:func:`LightGbmRegressor <nimbusml.ensemble.LightGbmRegressor>`

    .. index:: ensemble, booster

    Example:
       .. literalinclude:: /../nimbusml/examples/LightGbmClassifier.py
              :language: python
    """

    @trace
    def __init__(
            self,
            drop_rate=0.1,
            max_drop=1,
            skip_drop=0.5,
            xgboost_dart_mode=False,
            uniform_drop=False,
            unbalanced_sets=False,
            min_split_gain=0.0,
            max_depth=0,
            min_child_weight=0.1,
            subsample_freq=0,
            subsample=1.0,
            feature_fraction=1.0,
            reg_lambda=0.01,
            reg_alpha=0.0,
            scale_pos_weight=1.0,
            **params):
        core.__init__(
            self,
            drop_rate=drop_rate,
            max_drop=max_drop,
            skip_drop=skip_drop,
            xgboost_dart_mode=xgboost_dart_mode,
            uniform_drop=uniform_drop,
            unbalanced_sets=unbalanced_sets,
            min_split_gain=min_split_gain,
            max_depth=max_depth,
            min_child_weight=min_child_weight,
            subsample_freq=subsample_freq,
            subsample=subsample,
            feature_fraction=feature_fraction,
            reg_lambda=reg_lambda,
            reg_alpha=reg_alpha,
            scale_pos_weight=scale_pos_weight,
            **params)

    def get_params(self, deep=False):
        """
        Get the parameters for this operator.
        """
        return core.get_params(self)
