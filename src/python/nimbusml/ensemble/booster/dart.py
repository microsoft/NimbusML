# --------------------------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------------------------
# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
Dart
"""

__all__ = ["Dart"]


from ...internal.core.ensemble.booster.dart import Dart as core
from ...internal.utils.utils import trace


class Dart(core):
    """

    Dropouts meet Multiple Additive Regresion Trees.

    .. remarks::
        `Multiple Additive Regression Trees (MART)
        <https://arxiv.org/abs/1505.01866>`_ is an
        ensemble method of boosted regression trees. The Dropouts meet
        Multiple Additive Regression
    Trees (DART) employs dropouts in MART and overcomes the issues of over-
        specialization of MART,
    achiving better performance in many tasks.


        **Reference**

            `https://arxiv.org/abs/1505.01866
            <https://arxiv.org/abs/1505.01866>`_


    :param tree_drop_fraction: The drop ratio for trees. Range:(0,1).

    :param maximum_number_of_dropped_trees_per_round: Maximum number of dropped
        trees in a boosting round.

    :param skip_drop_fraction: Probability for not dropping in a boosting
        round.

    :param xgboost_dart_mode: True will enable xgboost dart mode.

    :param uniform_drop: True will enable uniform drop.

    :param minimum_split_gain: Minimum loss reduction required to make a
        further partition on a leaf node of the tree. the larger, the more
        conservative the algorithm will be.

    :param maximum_tree_depth: Maximum depth of a tree. 0 means no limit.
        However, tree still grows by best-first.

    :param minimum_child_weight: Minimum sum of instance weight(hessian) needed
        in a child. If the tree partition step results in a leaf node with the
        sum of instance weight less than min_child_weight, then the building
        process will give up further partitioning. In linear regression mode,
        this simply corresponds to minimum number of instances needed to be in
        each node. The larger, the more conservative the algorithm will be.

    :param subsample_frequency: Subsample frequency for bagging. 0 means no
        subsample. Specifies the frequency at which the bagging occurs, where
        if this is set to N, the subsampling will happen at every N
        iterations.This must be set with Subsample as this specifies the amount
        to subsample.

    :param subsample_fraction: Subsample ratio of the training instance.
        Setting it to 0.5 means that LightGBM randomly collected half of the
        data instances to grow trees and this will prevent overfitting. Range:
        (0,1].

    :param feature_fraction: Subsample ratio of columns when constructing each
        tree. Range: (0,1].

    :param l2_regularization: L2 regularization term on weights, increasing
        this value will make model more conservative.

    :param l1_regularization: L1 regularization term on weights, increase this
        value will make model more conservative.

    :param params: Additional arguments sent to compute engine.

    .. seealso::
        :py:func:`Gbdt <nimbusml.ensemble.booster.Gbdt>`,
        :py:func:`Goss <nimbusml.ensemble.booster.Goss>`,
        :py:func:`LightGbmBinaryClassifier
        <nimbusml.ensemble.LightGbmBinaryClassifier>`,
        :py:func:`LightGbmClassifier <nimbusml.ensemble.LightGbmClassifier>`,
        :py:func:`LightGbmRanker <nimbusml.ensemble.LightGbmRanker>`,
        :py:func:`LightGbmRegressor <nimbusml.ensemble.LightGbmRegressor>`

    .. index:: ensemble, booster

    Example:
       .. literalinclude:: /../nimbusml/examples/LightGbmClassifier.py
              :language: python
    """

    @trace
    def __init__(
            self,
            tree_drop_fraction=0.1,
            maximum_number_of_dropped_trees_per_round=1,
            skip_drop_fraction=0.5,
            xgboost_dart_mode=False,
            uniform_drop=False,
            minimum_split_gain=0.0,
            maximum_tree_depth=0,
            minimum_child_weight=0.1,
            subsample_frequency=0,
            subsample_fraction=1.0,
            feature_fraction=1.0,
            l2_regularization=0.01,
            l1_regularization=0.0,
            **params):
        core.__init__(
            self,
            tree_drop_fraction=tree_drop_fraction,
            maximum_number_of_dropped_trees_per_round=maximum_number_of_dropped_trees_per_round,
            skip_drop_fraction=skip_drop_fraction,
            xgboost_dart_mode=xgboost_dart_mode,
            uniform_drop=uniform_drop,
            minimum_split_gain=minimum_split_gain,
            maximum_tree_depth=maximum_tree_depth,
            minimum_child_weight=minimum_child_weight,
            subsample_frequency=subsample_frequency,
            subsample_fraction=subsample_fraction,
            feature_fraction=feature_fraction,
            l2_regularization=l2_regularization,
            l1_regularization=l1_regularization,
            **params)

    def get_params(self, deep=False):
        """
        Get the parameters for this operator.
        """
        return core.get_params(self)
