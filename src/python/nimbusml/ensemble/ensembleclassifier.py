# --------------------------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------------------------
# - Generated by tools/entrypoint_compiler.py: do not edit by hand
"""
EnsembleClassifier
"""

__all__ = ["EnsembleClassifier"]


from sklearn.base import ClassifierMixin

from ..base_predictor import BasePredictor
from ..internal.core.ensemble.ensembleclassifier import \
    EnsembleClassifier as core
from ..internal.utils.utils import trace
from .feature_selector import AllFeatureSelector
from .subset_selector import BootstrapSelector


class EnsembleClassifier(core, BasePredictor, ClassifierMixin):
    """

    **Description**
    Train a multi class ensemble model

    .. remarks::



    :param feature: see `Columns </nimbusml/concepts/columns>`_.

    :param label: see `Columns </nimbusml/concepts/columns>`_.

    :param sampling_type: .

    :param num_models: .

    :param sub_model_selector_type: :output_combiner:.

    :param output_combiner: Output combiner.

    :param normalize: Specifies the type of automatic normalization used:

        * ``"Auto"``: if normalization is needed, it is performed
          automatically. This is the default choice.
        * ``"No"``: no normalization is performed.
        * ``"Yes"``: normalization is performed.
        * ``"Warn"``: if normalization is needed, a warning
          message is displayed, but normalization is not performed.

        Normalization rescales disparate data ranges to a standard scale.
        Feature
        scaling ensures the distances between data points are proportional
        and
        enables various optimization methods such as gradient descent to
        converge
        much faster. If normalization is performed, a ``MinMax`` normalizer
        is
        used. It normalizes values in an interval [a, b] where ``-1 <= a <=
        0``
        and ``0 <= b <= 1`` and ``b - a = 1``. This normalizer preserves
        sparsity by mapping zero to zero.

    :param caching: Whether trainer should cache input training data.

    :param train_parallel: All the base learners will run asynchronously if the
        value is true.

    :param batch_size: Batch size.

    :param show_metrics: True, if metrics for each model need to be evaluated
        and shown in comparison table. This is done by using validation set if
        available or the training set.

    :param params: Additional arguments sent to compute engine.

    .. seealso::
        :py:class:`EnsembleRegressor
        <nimbusml.ensemble.EnsembleRegressor>`,
        :py:class:`EnsembleClassifier
        <nimbusml.ensemble.EnsembleClassifier>`


    .. index:: models, linear, SDCA, stochastic, classification, regression

    Example:
       .. literalinclude:: /../nimbusml/examples/FEnsembleClassifier.py
              :language: python
    """

    @trace
    def __init__(
            self,
            sampling_type=BootstrapSelector(
                feature_selector=AllFeatureSelector()),
            num_models=None,
            sub_model_selector_type=None,
            output_combiner=None,
            normalize='Auto',
            caching='Auto',
            train_parallel=False,
            batch_size=-1,
            show_metrics=False,
            feature=None,
            label=None,
            **params):

        if 'feature_column_name' in params:
            raise NameError(
                "'feature_column_name' must be renamed to 'feature'")
        if feature:
            params['feature_column_name'] = feature
        if 'label_column_name' in params:
            raise NameError(
                "'label_column_name' must be renamed to 'label'")
        if label:
            params['label_column_name'] = label
        BasePredictor.__init__(self, type='classifier', **params)
        core.__init__(
            self,
            sampling_type=sampling_type,
            num_models=num_models,
            sub_model_selector_type=sub_model_selector_type,
            output_combiner=output_combiner,
            normalize=normalize,
            caching=caching,
            train_parallel=train_parallel,
            batch_size=batch_size,
            show_metrics=show_metrics,
            **params)
        self.feature = feature
        self.label = label

    @trace
    def predict_proba(self, X, **params):
        '''
        Returns probabilities
        '''
        return self._predict_proba(X, **params)

    @trace
    def decision_function(self, X, **params):
        '''
        Returns score values
        '''
        return self._decision_function(X, **params)

    def get_params(self, deep=False):
        """
        Get the parameters for this operator.
        """
        return core.get_params(self)
