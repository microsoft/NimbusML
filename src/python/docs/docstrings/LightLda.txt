    """

    The LDA transform implements LightLDA, a state-of-the-art
    implementation of Latent Dirichlet Allocation.

    .. remarks::
        Latent Dirichlet Allocation is a well-known topic modeling algorithm
        that infers topical structure from text
        data, and can be used to featurize any text fields as low-dimensional
        topical vectors. LightLDA is an extremely
        efficient implementation of LDA developed in MSR-Asia that
        incorporates a number of optimization techniques
        `(http://arxiv.org/abs/1412.1576) <http://arxiv.org/abs/1412.1576>`_.
        With the LDA transform, we can
        train a topic model to produce 1 million topics with 1 million
        vocabulary on a 1-billion-token document set one
        a single machine in a few hours (typically, LDA at this scale takes
        days and requires large clusters). The most
        significant innovation is a super-efficient O(1) Metropolis-Hastings
        sampling algorithm, whose running cost is
        (surprisingly) agnostic of model size, allowing it to converges
        nearly an order of magnitude faster than other
        Gibbs samplers.


    .. seealso::
        :py:class:`NGramFeaturizer
        <nimbusml.feature_extraction.text.NGramFeaturizer>`,
        :py:class:`Sentiment
        <nimbusml.feature_extraction.text.Sentiment>`,
        :py:class:`WordEmbedding
        <nimbusml.feature_extraction.text.WordEmbedding>`.

    .. index:: transform, featurizer, text

    Example:
       .. literalinclude:: /../nimbusml/examples/LightLda.py
              :language: python
    """