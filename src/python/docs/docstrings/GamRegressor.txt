    """

    Generalized Additive Models

    .. remarks::
        `Generalized additive models
        <https://en.wikipedia.org/wiki/Generalized_additive_model>`_
        (referred
        to throughout as GAM) is a class of models expressable as an
        independent
        sum of individual functions. ``nimbusml``'s GAM learner comes in both
        binary
        classification (using logit-boosting) and regression (using least
        squares) flavors.

        In contrast to many formal definitions of GAM, this implementation
        found
        it convenient to represent learning over stepwise functions, which
        betrays the intention that GAM's components be smooth functions. In
        particular: the learner first discretizes features, and the "step"
        functions learned will step between the discretization boundaries.

        This implementation is based on the this `paper
        <https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.352.7619>`_,
        but diverges from it in several important respects: most
        significantly,
        in each round of boosting, rather than do one feature at a time, it
        instead makes a round on all features simultaneously. In each round,
        it
        will choose only one split point of each feature to change.

        In its current form, the GAM learner has the following advantages and
        disadvantages: on the one hand, they offer ready interpretability
        combined with expressive power, but on the other, they are currently
        slow. We would recommend their usage in the case where the key
        criteria
        is interpretability.

        Let's talk a bit more about interpretabilty. The next most
        interpretable
        model, we might say, is a linear model. But really, let's say that
        you
        have a feature with a coefficient of 3.9293, or something. What do
        you
        know? You know that generally, perhaps, larger values for that
        feature
        are "better." Great. But is 4 better than 3? Is 5 better than 4? To
        what
        degree? Are there "shapes" in the distributions hidden because of the
        reduction of a complex quantity to a single values? These are
        questions
        a linear model fundamentally cannot answer, but a GAM model might.
        

        **Reference**
    
            `Generalized additive models
            <https://en.wikipedia.org/wiki/Generalized_additive_model>`_,
            `Intelligible Models for Classification and Regression
            <https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.352.7619>`_
    

    :param normalize: Specifies the type of automatic normalization used:

        * ``"Auto"``: if normalization is needed, it is performed
          automatically. This is the default choice.
        * ``"No"``: no normalization is performed.
        * ``"Yes"``: normalization is performed.
        * ``"Warn"``: if normalization is needed, a warning
          message is displayed, but normalization is not performed.

        Normalization rescales disparate data ranges to a standard scale.
        Feature
        scaling insures the distances between data points are proportional
        and
        enables various optimization methods such as gradient descent to
        converge
        much faster. If normalization is performed, a ``MaxMin`` normalizer
        is
        used. It normalizes values in an interval [a, b] where ``-1 <= a <=
        0``
        and ``0 <= b <= 1`` and ``b - a = 1``. This normalizer preserves
        sparsity by mapping zero to zero.

    .. seealso::
        :py:func:`FastLinearRegressor
        <nimbusml.linear_model.FastLinearRegressor>`,
        :py:func:`OnlineGradientDescentRegressor
        <nimbusml.linear_model.OnlineGradientDescentRegressor>`
        

    .. index:: models, classification, svm

    Example:
       .. literalinclude:: /../nimbusml/examples/GamRegressor.py
              :language: python
    """
