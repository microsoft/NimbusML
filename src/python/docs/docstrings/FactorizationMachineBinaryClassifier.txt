    """

    Train a field-aware factorization machine for binary classification.

    .. remarks::
        Field Aware Factorization Machines use, in addition to the input
        variables, factorized
        parameters to model the interaction between pairs of variables. The
        algorithm is
        particularly useful for high dimensional datasets which can be very
        sparse (e.g.
        click-prediction for advertising systems). An advantage of FFM over
        SVMs is that the
        training data does not need to be stored in memory, and the
        coefficients can be optimized
        directly.



        **Reference**
    
            `Field Aware Factorization Machines
            <https://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf>`_,
            `Field-aware Factorization Machines for CTR Prediction
            <https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf>`_,
            `Adaptive Subgradient Methods for Online Learning and Stochastic
            Optimization
            <https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf>`_
    

    :param feature: see `Columns </nimbusml/concepts/columns>`_.

    :param label: see `Columns </nimbusml/concepts/columns>`_.

    :param normalize: Specifies the type of automatic normalization used:

        * ``"Auto"``: if normalization is needed, it is performed
          automatically. This is the default choice.
        * ``"No"``: no normalization is performed.
        * ``"Yes"``: normalization is performed.
        * ``"Warn"``: if normalization is needed, a warning
          message is displayed, but normalization is not performed.

        Normalization rescales disparate data ranges to a standard scale.
        Feature
        scaling insures the distances between data points are proportional
        and
        enables various optimization methods such as gradient descent to
        converge
        much faster. If normalization is performed, a ``MaxMin`` normalizer
        is
        used. It normalizes values in an interval [a, b] where ``-1 <= a <=
        0``
        and ``0 <= b <= 1`` and ``b - a = 1``. This normalizer preserves
        sparsity by mapping zero to zero.

    .. seealso::
        :py:func:`LogisticRegressionClassifier
        <nimbusml.linear_model.LogisticRegressionClassifier>`,
        :py:func:`AveragedPerceptronBinaryClassifier
        <nimbusml.linear_model.AveragedPerceptronBinaryClassifier>`,
        :py:func:`GamBinaryClassifier <nimbusml.ensemble.GamBinaryClassifier>`,
        :py:func:`SgdBinaryClassifier
        <nimbusml.linear_model.SgdBinaryClassifier>`

    .. index:: models, stochastic, online, gradient, descent

    Example:
      .. literalinclude:: /../nimbusml/examples/FactorizationMachineBinaryClassifier.py
             :language: python
    """